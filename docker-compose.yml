version: '3.8'
# transcription stream startup
#
# make sure the volumes exist
services:
  init:
    image: busybox
    container_name: file-system
    volumes:
      - transcriptionstream:/transcriptionstream
    command: /bin/sh -c "mkdir -p /transcriptionstream/incoming/transcribe /transcriptionstream/incoming/diarize /transcriptionstream/incoming/data /transcriptionstream/transcribed /transcriptionstream/scripts && chown -R transcriptionstream:transcriptionstream /transcriptionstream/incoming"


# Start up the worker container
  ts_transcription_service:
    image: ts-gpu:latest
    environment:
      - DIARIZATION_MODEL=${DIARIZATION_MODEL}
      - TRANSCRIPTION_MODEL=${TRANSCRIPTION_MODEL}
      - MAX_CONCURRENT_TRANSFORMS=${MAX_CONCURRENT_TRANSFORMS}
      - MAX_CONCURRENT_SUMMARYS=${MAX_CONCURRENT_SUMMARYS}
      - OLLAMA_ENDPOINT_IP=${OLLAMA_ENDPOINT_IP}
      - SALESDOCK_AUTHORIZATION=${SALESDOCK_AUTHORIZATION}
      - SALESDOCK_URL=${SALESDOCK_URL}
    shm_size: 6gb
    ports:
      - "22222:22"
    volumes:
      - transcriptionstream:/transcriptionstream
      - transcriptionstream:/home/transcriptionstream
#      - transcriptionstream-scripts:/root/scripts

    networks:
      ts_private_network:
        ipv4_address: 172.30.1.5

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

# Start up the web front end
  ts_web_service:
    image: ts-web
    environment:
      - TS_WEB_SECRET_KEY=${TS_WEB_SECRET_KEY}
      - TS_WEB_TOKEN=${TS_WEB_TOKEN}
      - SALESDOCK_AUTHORIZATION=${SALESDOCK_AUTHORIZATION}
      - SALESDOCK_URL=${SALESDOCK_URL}
    ports:
      - "5006:5000"
    volumes:
      - transcriptionstream:/transcriptionstream
    networks:
      ts_private_network:
        ipv4_address: 172.30.1.2


# if you want to run ollama locally and have enough vram uncomment this section
#  # Startup ts-gpt
  ts_gpt_service:
    image: ollama/ollama
    environment:
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL}
      - OLLAMA_MAX_LOADED_MODELS=${OLLAMA_MAX_LOADED_MODELS}
    ports:
      - "11434:11434"
    volumes:
      - transcriptionstream:/root/.ollama
    networks:
      ts_private_network:
        ipv4_address: 172.30.1.3

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]


networks:
  ts_private_network:
    ipam:
      config:
        - subnet: 172.30.0.0/16



volumes:
  transcriptionstream:
    external: true
#  transcriptionstream-scripts:
#    external: true

